---
title: "SBL Experiment Analysis"
output:
  html_notebook:
    toc: yes
---

------------------------------------------------------------------------

# Supplementary Material Chapter 6

**Author:** Michael S. Feurstein\
**Last compiled:** `r format(Sys.time(), '%d %B, %Y')`

------------------------------------------------------------------------

## About this notebook

This notebook is a supplementary material for Chapter 6 "A Controlled Experiment" used for replication of results. All steps and calculations have been validated through replication of a documented example in the [NCSS documentation Chapter 234](https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Analysis_of_2x2_Cross-Over_Designs_using_T-Tests.pdf), which uses data (again validated) from [Chow and Liu (1999)](https://www.routledge.com/Design-and-Analysis-of-Bioavailability-and-Bioequivalence-Studies/Chow-Liu/p/book/9781032477770). All further details on experiment design, methodology and analysis procedure can be found in Chapter 6 of thesis.

------------------------------------------------------------------------

## Preparations

We need several libraries for our analysis. Library ***dplyr*** is mainly used for data manipulation (e.g. summarise(), group_by()) and data transformation using pipes (e.g. %\>%) by magrittr. Library ***ggplot2*** is mainly used for additional plotting needs such as stacked bar plots, interaction plots and Tukey HSD confidence interval plots.

### Libraries

```{r}
library(tidyr)
library(dplyr)
library(ggplot2)
library(ggpubr)
library(lme4)
library(multcomp)
library(effectsize)
library(partR2)
library(moments)
library(lemon)
library(stringr)
library(sensR)
library(report)
```

### Load Data

Transformations and other preparation steps can be found in the [analysis.R](https://github.com/michaelfeurstein/sbl-experiment/blob/main/analysis.R) file. Here we simply load the already prepared data for the actual analysis steps performed.

The following imported file named ***"data_prepared.csv"*** contains all main measurements (duration, accuracy, sus-score).

```{r}
# read from prepared csv
df <- read.csv("data_prepared.csv")
```

The following is ***participant_profiles_anon_prepared.csv*** contains all anonymized data on participants and their profile (programming experience, programming interest, qualification etc.)

```{r}
# read from prepared csv
profiledata <- read.csv("participant_profiles_anon_prepared.csv")
```

The following imported file named ***"accuracy_features_flat_prepared.csv"*** contains all detailed measurements of the feature accuracy (1st part of the calculation for accuracy).

```{r}
# read from prepared csv
f_data <- read.csv("accuracy_features_flat_prepared.csv")
```

The following imported file named ***"accuracy_quality_flat_prepared.csv"*** contains all detailed measurements of the quality accuracy (2nd part of the calculation for accuracy).

```{r}
# read from prepared csv
q_data <- read.csv("accuracy_quality_flat_prepared.csv")
```

The following imported files named ***"sus_cnl_likert_tidy.csv"*** and ***"sus_kv_likert_tidy.csv"*** contain the Likert responses from the Standardized System Usability Scale (SUS). Note that these values are **NOT** to be confused with the calculated SUS scores. These are the 5-point Likert responses ranging from Strongly Disagree to Strongly Agree.

Importing CNL SUS responses

```{r}
# read CNL SUS responses from prepared tidy csv
sus_cnl_data_tidy <- read.csv("sus_details/sus_cnl_likert_tidy.csv")
# remove the imported X column - we don't need it. It was created during write.csv process
sus_cnl_data_tidy <- subset(sus_cnl_data_tidy, select = -c(X))
```

Importing KV SUS responses

```{r}
# read KV SUS responses from prepared tidy csv
sus_kv_data_tidy <- read.csv("sus_details/sus_kv_likert_tidy.csv")
# remove the imported X column - we don't need it. It was created during write.csv process
sus_kv_data_tidy <- subset(sus_kv_data_tidy, select = -c(X))
```

The following imported file named ***"ranking_preference.csv"*** contains the personal preference for a specific notation (CNL, KV or no preference). If a participant specified "don't know" or ranked both notations on the same level (1st or 2nd rank) the data point was converted to no preference.

```{r}
dfr <- read.csv("ranking_preference.csv", sep = ";", dec = ",", header = TRUE)
```

## Descriptive Statistics

The following steps are in line with the chronological order of Chapter 6, Section 6.3.1 Descriptive Statistics.

### Participants

#### Qualification

```{r}
# produce a table from Qual_fac
qual_table <- table(profiledata$Qual_fac)
# use prop.table to calcualte percentages
qual_table_prop <- prop.table(qual_table)
# number count
qual_table
# fractions
qual_table_prop
```

#### Occupation

```{r}
occu_table <- table(profiledata$Occu_fac)
occu_table_prop <- round((prop.table(occu_table) * 100),2)

# generate boxplot
par(mar=c(3, 8, 3, 0))
bp <- barplot(occu_table, main="Occupation",horiz=T , las=1, xlim=c(0,13))
text(occu_table, bp,  occu_table_prop, labels = paste(occu_table_prop, '%'), pos = 4, cex = 0.8)
```

#### Profile

```{r}
# Participant Profiles ####
authors <- colSums(profiledata == "Yes")
profile_columns = c("Profile","Frequency") 
df_p = data.frame(matrix(nrow = 0, ncol = length(profile_columns))) 
colnames(df_p) = profile_columns
df_p[nrow(df_p) + 1,] = c("Author",authors["Author"])
df_p[nrow(df_p) + 1,] = c("Researcher",authors["Researcher"])
df_p[nrow(df_p) + 1,] = c("User",authors["User"])
df_p[nrow(df_p) + 1,] = c("Tutor",authors["Tutor"])
df_p[nrow(df_p) + 1,] = c("Student",authors["Student"])

# convert frequency column into numeric format
df_p$Frequency = as.numeric(as.character(df_p$Frequency))

# order dataframe ascending
# this way it's displayed descending in horizontal barplot
desc_df_p <- df_p[order(df_p$Frequency),] 

# plot barplot
counts <- table(df_p$Frequency)
par(mar=c(3, 6, 3, 1))
profile_plot <- barplot(height=desc_df_p$Frequency, names=desc_df_p$Profile, main="Participant Profiles", horiz=T,las=1,xlim=c(0,25),width = c(2,2,2,2,2))
text(x=desc_df_p$Frequency, profile_plot, labels = paste(desc_df_p$Frequency), pos=4, offset=0.3, xpd=T)
```

### Dependent Variables

#### Efficiency (Duration)

**Boxplot:** Visual inspection of data, using a boxplot.

```{r}
##### boxplot ####
par(mar=c(5, 5, 3, 2))
bp_duration <- boxplot(duration.r ~ notation.r, main = "Duration", data = df, ylab = "Duration (in min.)", xlab = "Notation", names = c("KV","CNL"))
```

**Summary:** For reporting of mean (min, q1, mdn, q2, max) and sd:

```{r}
# using measured duration in minutes
df %>%
  group_by(notation.r) %>%
  summarize(mean_duration = mean(duration.r), sd_duration = sd(duration.r), min_duration = min(duration.r), max_duration = max(duration.r), med_duration = median(duration.r), q1 = quantile(duration.r, 0.25), q3 = quantile(duration.r, 0.75))
```

**Mean Difference:** For reporting of unstandardized mean difference:

```{r}
##### mean difference ####
# values
m.nl <- mean(df$duration.r[df$notation.r == "natural language"])
m.kv <- mean(df$duration.r[df$notation.r == "key-value"])
# unstandardized mean difference between cnl kv
diffnlkv <- m.nl-m.kv
# avergae of nl kv
avgnlkv <- (m.nl+m.kv)/2
rationlkv <- diffnlkv/avgnlkv
# percentage
percent_diff_nlkv <- rationlkv*100
print(c("mean difference in percentage: ", percent_diff_nlkv))
```

#### Effectiveness (Accuracy)

**Boxplot:** Visual inspection of data, using a boxplot.

```{r}
par(mar=c(5, 5, 3, 2))
boxplot(accuracy ~ notation.r, main = "Accuracy", data = df, xlab = "Notation", ylab = "Accuracy (in percent)", names = c("KV","CNL"))
```

**Summary:** For reporting of mean (min, q1, mdn, q2, max) and sd:

```{r}
df %>%
  group_by(notation.r) %>%
  summarize(mean = mean(accuracy), sd = sd(accuracy), min = min(accuracy), max = max(accuracy), med = median(accuracy), q1 = quantile(accuracy, 0.25), q3 = quantile(accuracy, 0.75))
```

Additional reports on proportions of 0% and 100% accuracy

```{r}
## percentages of 0% accuracy and 100% accuracy
cnl <- table(df$accuracy[df$notation.r == "natural language"])
cnl_p <- prop.table(cnl)

# number count
cnl
# fractions
cnl_p

kv <- table(df$accuracy[df$notation.r == "key-value"])
kv_p <- prop.table(min_table)

# number count
kv
# fractions
kv_p
```

**Mean Difference:** For reporting of unstandardized mean difference:

```{r}
##### mean difference ####
# values
accuracy_nl <- mean(df$accuracy[df$notation.r == "natural language"])
accuracy_kv <- mean(df$accuracy[df$notation.r == "key-value"])
# difference between nl kv: nl-kv
accuracy_diffnlkv <- accuracy_nl-accuracy_kv
# avergae of nl kv
accuracy_avgnlkv <- (accuracy_nl+accuracy_kv)/2
accuracy_rationlkv <- accuracy_diffnlkv/accuracy_avgnlkv
# percentage
accuracy_percent_diff_nlkv <- abs(accuracy_rationlkv*100)
print(c("mean difference in percentage: ", accuracy_percent_diff_nlkv))
```

**Stacked Bar plots:** Detailed Accuracy Feature

Following the regular descriptive reports, here we present detailed descriptive data about the number of features being implemented or not. This data is not included in the hypothesis tests or in any research questions. It simply shows where "errors" occured or things went wrong in terms of not implementing features or not adhering to quality standards.

Note that the facet labels are referenced from Table 6.2 (thesis; see also [github/sbl-experiment/docs/features.md](https://github.com/michaelfeurstein/sbl-experiment/blob/main/docs/features.md))

```{r}
# New facet label names for dose variable
feature_f.labels <- c("F1[a]", "F2[a]", "F2[b]", "F2[c]", "F2[d]", "F2[e]", "F3[a]", "F3[b]", "F4[a]", "F4[b]", "F4[c]", "F4[d]", "F4[e]", "F4[f]", "F4[g]", "F5[a]", "F5[b]")
names(feature_f.labels) <- c("f1a", "f2a", "f2b", "f2c", "f2d", "f2e", "f3a", "f3b", "f4a", "f4b", "f4c", "f4d", "f4e", "f4f", "f4g", "f5a", "f5b")

# Draw barplot with grouping & stacking
ggplot(f_data,
        aes(x = notation, y = value, fill = accuracy)) +
        geom_bar(stat = "identity", position = "stack") +
        facet_grid(cols = vars(feature), labeller = labeller(feature = feature_f.labels, .default = label_parsed)) +
  scale_y_continuous(labels = function(x) paste0(x, "%")) +
  scale_x_discrete(labels = c("kv","cnl")) +
  scale_fill_manual(values = c("#91cf60","#fc8d59")) +
  geom_text(aes(label = paste0(round(value, digits = 0),"%")),
            position = position_stack(),
            hjust = 1.2,
            vjust = 0.5,
            angle = 90,
            size = 2.5) + 
  ggtitle("Detailed accuracy (in %) for features implemented") +
  xlab("Notation") +
  ylab("Accuracy") +
  labs(fill="Feature\nimplemented") +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```

**Stacked Bar plots:** Detailed Accuracy Quality

```{r}
# New facet label names for dose variable
feature_q.labels <- c("Q[1]", "Q[2]", "Q[3]", "Q[4]", "Q[5]", "Q[6]")
names(feature_q.labels) <- c("q1", "q2", "q3", "q4", "q5", "q6")

# plot
ggplot(q_data,
        aes(x = notation, y = value, fill = accuracy)) +
        geom_bar(stat = "identity", position = "stack") +
        facet_grid(cols = vars(feature), labeller = labeller(feature = feature_q.labels, .default = label_parsed)) +
        scale_y_continuous(labels = function(x) paste0(x, "%")) +
        scale_x_discrete(labels = c("kv","cnl")) +
        scale_fill_manual(values = c("#91cf60","#fc8d59")) +
        geom_text(aes(label = paste0(round(value, digits = 0),"%")),
            position = position_stack(),
            hjust = 1.2,
            vjust = 0.5,
            angle = 90,
            size = 3) + 
  ggtitle("Detailed accuracy (in %) for quality criteria compliance") +
  xlab("Notation") +
  ylab("Accuracy") +
  labs(fill="Quality Criteria\ncompliant")
```

#### Usability (SUS Score)

**Boxplot:** Visual inspection of data, using a boxplot.

```{r}
par(mar=c(5, 5, 3, 2))
boxplot(sus ~ notation.r, data = df, main = "Usability (System Usability Scale)", xlab = "Notation", ylab = "SUS score", names = c("KV","CNL"))
```

**Summary:** For reporting of mean (min, q1, mdn, q2, max) and sd:

```{r}
df %>%
  group_by(notation.r) %>%
  summarize(mean = mean(sus), sd = sd(sus), min = min(sus), max = max(sus), med = median(sus), q1 = quantile(sus, 0.25), q3 = quantile(sus, 0.75))
```

**Mean Difference:** For reporting of unstandardized mean difference:

```{r}
##### mean difference ####
# values
sus_nl <- mean(df$sus[df$notation.r == "natural language"])
sus_kv <- mean(df$sus[df$notation.r == "key-value"])
# difference between nl kv: nl-kv
sus_diffnlkv <- sus_nl-sus_kv
# avergae of nl kv
sus_avgnlkv <- (sus_nl+sus_kv)/2
sus_rationlkv <- sus_diffnlkv/sus_avgnlkv
# percentage
sus_percent_diff_nlkv <- abs(sus_rationlkv*100)
print(c("mean difference in percentage: ", sus_percent_diff_nlkv))
```

**Stacked Bar plots:** Detailed response results to the individual SUS questions

The following stacked bar plots have been used and reported by Ivanchikj et al. (2020) (see [References] and [DOI](https://doi.org/10.1145/3365438.3410990)). For the purpose of this thesis the style and reporting has been replicated with our data.

**Stacked Bar plots:** Likert Responses for Usability Questionnaire on Controlled-Natural Language Notation

```{r}
# First we use mutate and factor to bring questions (PQ and NQ) into desired order for plotting
sus_cnl_data_tidy <- sus_cnl_data_tidy %>% mutate(question = factor(question, 
    levels = c("PQ5. I would feel very confident using the notation.",
               "PQ4. I would imagine that most people would learn to use this notation very quickly.",
               "PQ3. I find the various functions in this notation are well integrated.",
               "PQ2. I think the notation would be easy to use.",
               "PQ1. I think that I would like to use this notation frequently.",
               "NQ5. I would need to learn a lot of things before I could get going with this notation.",
               "NQ4. I would find the notation very cumbersome to use.",
               "NQ3. I think there is too much inconsistency in this notation.",
               "NQ2. I think that I would need the support of a technical person to be able to use this notation.",
               "NQ1. I find the notation unnecessarily complex.")))

# Then we factorize the Likert answer scale
sus_cnl_data_tidy <- sus_cnl_data_tidy %>% mutate(answer = factor(answer, 
               levels = c('Strongly Disagree','Disagree','Neutral','Agree','Strongly Agree')))

# In preparation for plotting we setup a function to define a limit width for strings wrapped
# Source juliasilge (comment on github from Oct 4, 2022)
# Link: https://github.com/juliasilge/tidytext/issues/222
custom_labeler <- function(x) {
  x %>%
    str_replace("___.+$", "") %>%
    str_wrap(width = 50)
}

# The actual plot code using ggplot2
ggplot(sus_cnl_data_tidy, aes(x=question, fill=answer)) + 
  geom_bar(width = 0.7, position = position_stack(reverse = TRUE)) + 
  scale_fill_manual(values=c("darkred","red", "grey", "darkolivegreen1", "darkgreen")) +
  scale_y_continuous(expand = expansion(0)) +
  theme_bw() +
  theme(axis.text.y=element_text(hjust=0), axis.title.y = element_blank(), axis.title.x =element_blank(), legend.position = "bottom", legend.justification = c(1,1), legend.title = element_blank()) +
  facet_rep_grid(notion ~ ., scales = "free", repeat.tick.labels = "all") +
  scale_x_discrete(labels = custom_labeler) +
  coord_capped_cart(bottom="both", left="both") +
  coord_flip()
```

**Stacked Bar plots:** Likert Responses for Usability Questionnaire on Key-Value Notation

```{r}
# First we use mutate and factor to bring questions (PQ and NQ) into desired order for plotting

sus_kv_data_tidy <- sus_kv_data_tidy %>% mutate(question = factor(question, 
    levels = c("PQ5. I would feel very confident using the notation.",
               "PQ4. I would imagine that most people would learn to use this notation very quickly.",
               "PQ3. I find the various functions in this notation are well integrated.",
               "PQ2. I think the notation would be easy to use.",
               "PQ1. I think that I would like to use this notation frequently.",
               "NQ5. I would need to learn a lot of things before I could get going with this notation.",
               "NQ4. I would find the notation very cumbersome to use.",
               "NQ3. I think there is too much inconsistency in this notation.",
               "NQ2. I think that I would need the support of a technical person to be able to use this notation.",
               "NQ1. I find the notation unnecessarily complex.")))

# Then we factorize the Likert answer scale
sus_kv_data_tidy <- sus_kv_data_tidy %>% mutate(answer = factor(answer, 
              levels = c('Strongly Disagree','Disagree','Neutral','Agree','Strongly Agree')))

# In preparation for plotting we setup a function to define a limit width for strings wrapped
# Source juliasilge (comment on github from Oct 4, 2022)
# Link: https://github.com/juliasilge/tidytext/issues/222
custom_labeler <- function(x) {
  x %>%
    str_replace("___.+$", "") %>%
    str_wrap(width = 50)
}

# The actual plot code using ggplot2
ggplot(sus_kv_data_tidy, aes(x=question, fill=answer)) + 
  geom_bar(width = 0.7, position = position_stack(reverse = TRUE)) + 
  scale_fill_manual(values=c("darkred","red", "grey", "darkolivegreen1", "darkgreen")) +
  scale_y_continuous(expand = expansion(0)) +
  theme_bw() +
  theme(axis.text.y=element_text(hjust=0), axis.title.y = element_blank(), axis.title.x =element_blank(), legend.position = "bottom", legend.justification = c(1,1), legend.title = element_blank()) +
  facet_rep_grid(notion ~ ., scales = "free", repeat.tick.labels = "all") +
  scale_x_discrete(labels = custom_labeler) +
  coord_capped_cart(bottom="both", left="both") +
  coord_flip()

```

#### Ranking (Personal Preference)

**Barplot:** Visual inspection of data, using a barplot.

```{r}
# Define a desired order manually
desired_order <- c("preferNL","nopref","preferKV")
# Reorder levels
dfr$preference <- factor( as.character(dfr$preference), levels=desired_order )
# Reorder dataframe
dfr <- dfr[order(dfr$preference),]

# Prepare counts for barplot
counts <- table(dfr$preference)

# Barplot
b<-barplot(counts, main="Ranking (Personal Preference)",
        xlab="Preference for a notation",names.arg=c("CNL", "No Preference", "KV"),ylim=range(pretty(c(0, counts))))

# Set text inside barplots
text(b, counts - 1.5, paste0(sprintf("%4.1f ", counts / sum(counts) * 100), "%", sprintf(" (%d)",counts)), font=1, col=c("black"), cex = 0.9)
```

## Results and Hypothesis Tests

The following steps are in line with the chronological order of Chapter 6, Section 6.3.2 Results and Hypothesis Tests.

**Note on period difference calculation**

For the analysis of the effects of the two treatments, the difference of the measurements taken in Period 1 and 2 are used (Chapter 6, Section 6.1.8). We calculate the period differences for each sequence. This step has been validated with calculations in [Validate SBL Experiment Procedure using Chow and Liu (1999)](https://github.com/michaelfeurstein/sbl-experiment/blob/main/validation/chowliu73/chowliu73_validation.Rmd).

```{r}
# TODO move to Preparations 
# TODO merge into one df when all variables clear
# period differences: duration
p1 <- df %>%
  group_by(subject, sequence) %>%
  summarize(duration_difference = duration.log[1]-duration.log[2])

p1 %>%
  group_by(sequence) %>%
  summarize(mean = mean(duration_difference))

# period differences: accuracy
p2 <- df %>%
  group_by(subject, sequence) %>%
  summarize(accuracy_difference = accuracy[1]-accuracy[2])

# period differences: usability
p3 <- df %>%
  group_by(subject, sequence) %>%
  summarize(sus_difference = sus.boxcox[1]-sus.boxcox[2])

temp_df <- merge(x = p1, y = p2[ , c("subject", "accuracy_difference")], by = "subject", all.x=TRUE)

df_calc <- merge(x = temp_df, y = p3[ , c("subject", "sus_difference")], by = "subject", all.x=TRUE)

p2 %>%
  group_by(sequence) %>%
  summarize(acc_diff_mean = mean(accuracy_difference))
```

### Tests of Normality

#### Efficiency (Duration)

Two types of visual inspections (histograms and qq-plots) and one type of statistical test (Shapiro-Wilk Test).

**Result:** Data is not normal. We use log transformation on duration.r to get normal distribution (duration.log).

##### Histogram

```{r}
##### histogram ####
hist(df$duration.r[df$notation.r == "natural language"], main="CNL Duration without transformation", xlab="Duration (in minutes)", xlim = range(pretty(c(0, 60))), ylim = range(pretty(c(0,20))))
hist(df$duration.log[df$notation.r == "natural language"], main="CNL Duration transformed", xlab="Duration (in minutes)", xlim = range(pretty(c(1, 4))), ylim = range(pretty(c(0,25))))

hist(df$duration.r[df$notation.r == "key-value"], main="KV Duration without transformation", xlab="Duration (in minutes)", xlim = range(pretty(c(0, 30))), ylim = range(pretty(c(0,30))))
hist(df$duration.log[df$notation.r == "key-value"], main="KV Duration transformed", xlab="Duration (in minutes)", xlim = range(pretty(c(1, 4))), ylim = range(pretty(c(0,10))))
```

##### QQ-Plots

Normal probability plot of residuals for controlled natural language notation with log transformed duration.

```{r}
qqnorm(df$duration.log[df$notation.r == "natural language"], pch = 1, frame = FALSE, main = "CNL Normal Q-Q Plot for duration")
qqline(df$duration.log[df$notation.r == "natural language"], col = "steelblue", lwd = 2)
```

Normal probability plot of residuals for key-value notation with log transformed duration.

```{r}
qqnorm(df$duration.log[df$notation.r == "key-value"], pch = 1, frame = FALSE, main = "KV Normal Q-Q Plot for duration")
qqline(df$duration.log[df$notation.r == "key-value"], col = "steelblue", lwd = 2)
```

##### Shapiro-Wilk Test

Using period difference, p-values are not significant, thus we cannot reject the null hypothesis which states that the data has normal distribution. **Result: Data is normal; use parametric tests.**

```{r}
shapiro.test(df_calc$duration_difference[df_calc$sequence == "KV-NL"])
shapiro.test(df_calc$duration_difference[df_calc$sequence == "NL-KV"])
```

**Shapiro-Wilk Test**

Using treatments, p-values are not significant, thus we cannot reject the null hypothesis which states that the data has normal distribution. **Result: Data is normal; use parametric tests.**

```{r}
shapiro.test(df$duration.log[df$notation.r == "natural language"])
shapiro.test(df$duration.log[df$notation.r == "key-value"])
```

#### Effectiveness (Accuracy)

Two types of visual inspections (histograms and qq-plots) and two types of statistical tests (Shapiro-Wilk and Skewness-Test).

**Result:** Data is not normal. Data is skewed. No transformation works (log or box cox).

##### Histogram

```{r}
##### histogram ####
hist(df$accuracy[df$notation.r == "natural language"], main="CNL Accuracy without transformation", xlab="Accuracy (in percent)", ylim = range(pretty(c(0,30))))

hist(df$accuracy[df$notation.r == "key-value"], main="KV Accuracy without transformation", xlab="Accuracy (in percent)", ylim = range(pretty(c(0,40))))
```

##### QQ-Plots

Normal probability plot of residuals for controlled natural language notation for accuracy (no transformation).

```{r}
qqnorm(df$accuracy[df$notation.r == "natural language"], pch = 1, frame = FALSE, main = "CNL Normal Q-Q Plot for accuracy")
qqline(df$accuracy[df$notation.r == "natural language"], col = "steelblue", lwd = 2)
```

Normal probability plot of residuals for key-value notation for accuracy (no transformation).

```{r}
qqnorm(df$accuracy[df$notation.r == "key-value"], pch = 1, frame = FALSE, main = "KV Normal Q-Q Plot for accuracy")
qqline(df$accuracy[df$notation.r == "key-value"], col = "steelblue", lwd = 2)
```

##### Shapiro-Wilk Test

Using period difference, p-values are significant (only KV-NL), still we go with the worst case and reject the null hypothesis which states that the data is of normal distribution. **Result: data is not normal; use non-parametric tests.**

```{r}
shapiro.test(df_calc$accuracy_difference[df_calc$sequence == "KV-NL"])
shapiro.test(df_calc$accuracy_difference[df_calc$sequence == "NL-KV"])
```

**Shapiro-Wilk Test**

Using treatments, p-values are significant, thus we reject the null hypothesis which states that the data is of normal distribution. **Result: data is not normal; use non-parametric tests.**

```{r}
shapiro.test(df$accuracy[df$notation.r == "natural language"])
shapiro.test(df$accuracy[df$notation.r == "key-value"])
```

#### Usability (SUS Score)

Two types of visual inspections (histograms and qq-plots) and two types of statistical tests (Shapiro-Wilk and Skewness-Test).

**Result:** Data is not normal. Boxcox transformation is used.

##### Histogram

```{r}
##### histogram ####
hist(df$sus[df$notation.r == "natural language"], main="CNL SUS scores without transformation", xlab="SUS Score", ylim = range(pretty(c(0,20))), xlim = range(pretty(c(0,100))))
hist(df$sus.boxcox[df$notation.r == "natural language"], main="CNL SUS scores transformed", xlab="SUS Score (transformed)", ylim = range(pretty(c(0,10))))

hist(df$sus[df$notation.r == "key-value"], main="KV SUS scores without transformation", xlab="SUS Score", ylim = range(pretty(c(0,15))), xlim = range(pretty(c(0,100))))
hist(df$sus.boxcox[df$notation.r == "key-value"], main="KV SUS scores transformed", xlab="SUS Score (transformed)", ylim = range(pretty(c(0,15))))
```

##### QQ-Plots

Normal probability plot of residuals for controlled natural language notation with boxcox transformed SUS scores.

```{r}
qqnorm(df$sus.boxcox[df$notation.r == "natural language"], pch = 1, frame = FALSE, main = "CNL Normal Q-Q Plot for SUS scores")
qqline(df$sus.boxcox[df$notation.r == "natural language"], col = "steelblue", lwd = 2)
```

Normal probability plot of residuals for key-value notation with boxcox transformed SUS scores.

```{r}
qqnorm(df$sus.boxcox[df$notation.r == "key-value"], pch = 1, frame = FALSE, main = "KV Normal Q-Q Plot for SUS scores")
qqline(df$sus.boxcox[df$notation.r == "key-value"], col = "steelblue", lwd = 2)
```

##### Shapiro-Wilk Test

Using period difference, p-values are significant (only NL-KV), still we go with the worst case and reject the null hypothesis which states that the data is of normal distribution. **Result: data is not normal; use non-parametric tests.**

```{r}
shapiro.test(df_calc$sus_difference[df_calc$sequence == "KV-NL"])
shapiro.test(df_calc$sus_difference[df_calc$sequence == "NL-KV"])
```

**Shapiro-Wilk Test**

Using treatments, p-values are not significant, thus we cannot reject the null hypothesis which states that the data has normal distribution. **Result: data is normal; use parametric tests.**

```{r}
shapiro.test(df$sus.boxcox[df$notation.r == "natural language"])
shapiro.test(df$sus.boxcox[df$notation.r == "key-value"])
```

### Cross-over Analysis

Two types of inspection: (1) interaction plots using sum of means for notation-by-period; (2) paired two-sided t-tests to check for cross-over effects. Based on Jones & Kenward (2014).

#### Efficiency (Duration)

##### (1) Interaction plot

Notation-by-Period Means

```{r}
### CROSS-OVER ANALYSIS ####
# Testing carry-over and treatment effect with t-tests
# Based on: https://www.lexjansen.com/pharmasug/2006/Posters/PO16.pdf

#### duration  ####
# calculate and plot sum of means for notation,period
# using non-transformed duration
np <- df %>%
  group_by(notation.r, period) %>%
  summarize(mean_duration = mean(duration.r))

pd = position_dodge(0)
np.plot <- ggplot(np, aes(x = period,
               y = mean_duration,
               color = notation.r,
               group = notation.r)) +
  geom_point(shape  = 15,
             size   = 4,
             position = pd) +
  geom_line() + theme(
    legend.position = c(.99, .99),
    legend.justification = c("right", "top"),
    legend.box.just = "right",
    legend.margin = margin(4, 4, 4, 4),
    axis.title   = element_text(face = "bold"),
    axis.text    = element_text(face = "bold"),
    plot.caption = element_text(hjust = 0)
  ) +
  scale_x_continuous(limits = c(0.5, 2.5),breaks=seq(1,2),minor_breaks=NULL) +
  ylab("Duration mean") +
  xlab("Period") +
  labs(title = "Plot of Notation-by-Period Means for Duration", subtitle = "Period vs. response (duration) by treatment (notation)", color = "Notation")

# Currently only using notation,period interaction plot
np.plot
```

##### (2) T-Test

```{r}
t.test(mean_duration ~ notation.r, data = np, paired = TRUE, conf.level = 0.95, alternative = "two.sided")
```

#### Effectiveness (Accuracy)

##### (1) Interaction plot

Notation-by-Period Means

```{r}
#### accuracy  ####
# calculate and plot sum of means for notation,period
np <- df %>%
  group_by(notation.r, period) %>%
  summarize(mean_accuracy = mean(accuracy))

pd = position_dodge(0)
np.plot <- ggplot(np, aes(x = period,
                          y = mean_accuracy,
                          color = notation.r,
                          group = notation.r)) +
  geom_point(shape  = 15,
             size   = 4,
             position = pd) +
  geom_line() + theme(
    legend.position = c(.99, .99),
    legend.justification = c("right", "top"),
    legend.box.just = "right",
    legend.margin = margin(4, 4, 4, 4),
    axis.title   = element_text(face = "bold"),
    axis.text    = element_text(face = "bold"),
    plot.caption = element_text(hjust = 0)
  ) +
  scale_x_continuous(limits = c(0.5, 2.5),breaks=seq(1,2),minor_breaks=NULL) +
  ylab("Accuracy") +
  xlab("Period") +
  labs(title = "Plot of Notation-by-Period Means for Accuracy", subtitle = "Period vs. response (accuracy) by treatment (notation)", color = "Notation")

np.plot
```

##### (2) T-Test

```{r}
t.test(mean_accuracy ~ notation.r, data = np, paired = TRUE, conf.level = 0.95, alternative = "two.sided")
```

#### Usability (SUS Score)

##### (1) Interaction plot

Notation-by-Period Means

```{r}
#### sus  ####
# calculate and plot sum of means for notation,period
np <- df %>%
  group_by(notation.r, period) %>%
  summarize(mean_sus = mean(sus))

pd = position_dodge(0)
np.plot <- ggplot(np, aes(x = period,
                          y = mean_sus,
                          color = notation.r,
                          group = notation.r)) +
  geom_point(shape  = 15,
             size   = 4,
             position = pd) +
  geom_line() + theme(
    legend.position = c(.99, .99),
    legend.justification = c("right", "top"),
    legend.box.just = "right",
    legend.margin = margin(4, 4, 4, 4),
    axis.title   = element_text(face = "bold"),
    axis.text    = element_text(face = "bold"),
    plot.caption = element_text(hjust = 0)
  ) +
  scale_x_continuous(limits = c(0.5, 2.5),breaks=seq(1,2),minor_breaks=NULL) +
  ylab("SUS score") +
  xlab("Period") +
  labs(title = "Plot of Notation-by-Period Means for SUS scores", subtitle = "Period vs. response (SUS score) by treatment (notation)", color = "Notation")

np.plot
```

##### (2) T-Test

```{r}
t.test(mean_sus ~ notation.r, data = np, paired = TRUE, conf.level = 0.95, alternative = "two.sided")
```

### Treatment Effects

We report both t-tests, as recommended by Jones & Kenward (2014) and results from a linear mixed model, as recommended by Vegas et al. (2016). Additionally analysis steps are based on input by [Thomas Rusch](https://www.wu.ac.at/en/methods/team/dr-thomas-rusch/) from the Competence Center for Empirical Research Methods and details communicated by Lukas Meier in slideset on "[Multiple Treatments on the Same Experimental Unit](https://ethz.ch/content/dam/ethz/special-interest/math/statistics/sfs/Education/Advanced%20Studies%20in%20Applied%20Statistics/course-material/repeatedMeasures/4_crossover.pdf)".

For each variable we setup:

(1) linear mixed model\
(2) pair-wise comparison t-tests\
(3) effect size\
(4) Tukey HSD

[**Notes on linear mixed models**]{.underline}

We now setup a linear mixed models using lmer for each variable (duration, accuracy, usability).

*Note 1:* we don't fit a model as we don't really have a max/min model with options to remove random effects and compare the model fit to the previous model.

Note 2: we setup two models based on slideset by Lukas Meier and validation data replicated with chowliu (link? chowliu73_validation.Rmd) in order to check:

-   **period effect** is used from model based on sequence effect (model_seq)

-   **carryover effect** is used from model based on interaction effect (model_int)

-   **treatment effect** can be used from both models (model_seq = model_int)

TODO:

-   For reporting create data to show in table with also effect size and tukey plot on top of it. all in one. then write about it.

-   Link for checkmodel: <https://iamciera.github.io/lme4tutorial/>

#### Efficiency (Duration)

##### (1) Linear Mixed Model

[**Model 1: Sequence Effect**]{.underline}

```{r}
###### Linear Regression (Linear Mixed Model) for Duration ####
# based on input from Thomas Rusch + slideset from Meier
# model as a sequence effect
model_seq_dur<-lmer(duration.log~notation.r+period+sequence+(1|subject),data=df)
summary(model_seq_dur)
```

From the above we can see that there is a period effect. Note that H0 refers to the period effect.

| Parameter     | Estimate | Standard Error | T-Value | DF  | Prob Level | Reject H0 at α = 0.050? |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| Period Effect | -0.35476 | 0.04728        | -7.504  | 46  | 1.61e-09   | Yes                     |

[Report:]{.underline} The tests of fixed effects show a **significant period effect**. The test rejects the assumption of equal period effects at the 0.05 significance level (the actual probability level is 1.61e-09).

[**Model 2: Interaction Effect**]{.underline}

```{r}
# model as a interaction effect
model_int_dur<-lmer(duration.log~notation.r*period+(1|subject),data=df)
summary(model_int_dur)
```

From the above output we can see that there is no carryover effect (the carryover effect is leveled out by design and this is hereby confirmed:

| Parameter        | Estimate | Standard Error | T-Value | DF  | Prob Level | Reject H0 at α = 0.050? |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| Carryover Effect | -0.09427 | 0.19557        | -0.482  | 46  | 0.63210    | No                      |

[Report:]{.underline} The tests of fixed effects show **no significant carryover effect**. The test failed to reject the assumption of equal carryover effects at the 0.05 significance level (the actual probability level is 0.63210).

[**Treatment Effect: Duration**]{.underline}

| Alternative Hypothesis | Mean Difference in percent | Standard Error | T-Value | DF  | Prob Level | Reject H0 at α = 0.050? |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| μCNL - μKV ≠ 0         | 29.46605                   | 0.04728        | 5.690   | 46  | 8.42e-07   | Yes                     |

Report: The two treatment means are significantly different based on the reported fixed effects in the linear mixed model at the 0.05 significance level (the actual probability level was 8.42e-07).

##### (2) Effect Size

[**Unstandardized**]{.underline}

Based on Kampenes et al. (2007) "[u]nstandardized effect size measures are expressed in terms of raw units [...]" (p. 1077).

```{r}
##### mean difference (log transformed) ####
# values
m_duration_nl <- mean(df$duration.log[df$notation.r == "natural language"])
m_duration_kv <- mean(df$duration.log[df$notation.r == "key-value"])
# unstandardized mean difference between cnl kv
diff_duration_nlkv <- m_duration_nl-m_duration_kv
# avergae of nl kv
avg_duration_nlkv <- (m_duration_nl+m_duration_kv)/2
ratio_diff_duration_nlkv <- diff_duration_nlkv/avg_duration_nlkv
# percentage
percent_diff_duration_nlkv <- ratio_diff_duration_nlkv*100
print(c("Mean Difference Duration (log transformed)"))
print(c("mean difference duration: ", diff_duration_nlkv))
print(c("mean difference in percentage: ", percent_diff_duration_nlkv))

##### mean difference (measured values) ####
# values
m_duration_nl <- mean(df$duration.r[df$notation.r == "natural language"])
m_duration_kv <- mean(df$duration.r[df$notation.r == "key-value"])
# unstandardized mean difference between cnl kv
diff_duration_nlkv <- m_duration_nl-m_duration_kv
# avergae of nl kv
avg_duration_nlkv <- (m_duration_nl+m_duration_kv)/2
ratio_diff_duration_nlkv <- diff_duration_nlkv/avg_duration_nlkv
# percentage
percent_diff_duration_nlkv <- ratio_diff_duration_nlkv*100
print(c("Mean Difference Duration (measured in raw units: minutes)"))
print(c("mean difference duration: ", diff_duration_nlkv))
print(c("mean difference in percentage: ", percent_diff_duration_nlkv))
```

[Report:]{.underline} The estimated unstandardized mean difference (μCNL - μKV), measured in raw units (minutes) is 3.54 minutes, that is 29.47 %.

[**Standardized**]{.underline}

**Marginal/Conditional R2**

"*Marginal R2* considers only the variance of the **fixed effects** (without the random effects) and *conditional R2* takes *both* the **fixed and random effects** into account (i.e., the total model)." ([Source](https://easystats.github.io/performance/articles/r2.html#r2-for-mixed-models))

```{r}
# Understanding effect size for linear mixed models
# Source: https://joshuawiley.com/MonashHonoursStatistics/LMM_Comparison.html#effect-sizes

# Details on getting R-squared for mixed models
# Source: https://easystats.github.io/performance/articles/r2.html#r2-for-mixed-models

# Details on using partR2 to get CIs
# Source: https://cran.r-project.org/web/packages/partR2/vignettes/Using_partR2.html
# Paper: https://peerj.com/articles/11414/

# Interpretation of R-squared
# Source: https://easystats.github.io/effectsize/reference/interpret_r2.html

# R-squared (marginal/conditional)
r2_dur <- performance::r2(model_seq)
r2_dur
# Interpretation of R-suqared (marginal/conditional)
interpret_r2(c(r2_dur[["R2_conditional"]][["Conditional R2"]],r2_dur[["R2_marginal"]][["Marginal R2"]]), rules = "cohen1988")

# Using partR2 to confirm performance package and get CIs
# Marginal R-squared
r2_mar_dur <- partR2(model_seq, partvars = c("notation.r", "period"), 
                  R2_type = "marginal", nboot = 10)
r2_mar_dur
summary(r2_mar_dur)
forestplot(r2_mar_dur, type = "R2")

# Conditional R-squared
r2_con_dur <- partR2(model_seq, partvars = c("notation.r", "period"), 
                  R2_type = "conditional", nboot = 10)
r2_con_dur
summary(r2_con_dur)
forestplot(r2_con_dur, type = "R2")
```

**Cohen's d**

```{r}
effectsize::cohens_d(df$duration.log[df$notation.r == "natural language"],df$duration.log[df$notation.r == "key-value"], paired = TRUE, pooled_sd = TRUE)
```

##### (3) Posthoc: Tukey HSD

[**Tukey HSD**]{.underline}

Post-hoc test looking at contrast CNL-KV.

```{r}
posthoc_dur <- summary(glht(model_seq, linfct=mcp(notation.r="Tukey")))
ci_dur <- confint(model_seq, level = 0.95)

means <- tapply(df$duration.log, df$notation.r, mean)
mean_dur_diff <- means[2] - means[1]
mean_dur_diff <- mean_dur_diff[["natural language"]]
mean_dur_diff_lwr <- ci_dur[4,1]
mean_dur_diff_upr <- ci_dur[4,2]

mean_dur_diff_upr
mean_dur_diff
mean_dur_diff_lwr

# Plotting Tukey HSD
# Source: Stefan aov.html + http://sape.inf.usi.ch/quick-reference/ggplot2/geom_pointrange

# (1) using log transformed data
duration_transformed_CI=data.frame(contrast="nl-kv", lower=mean_dur_diff_lwr, mean=mean_dur_diff, upper=mean_dur_diff_upr)

ggplot() + 
  geom_pointrange(data=duration_transformed_CI, mapping=aes(x=contrast, y=mean, ymin=lower, ymax=upper), size=1, color="black", fill="white", shape=22) + 
  geom_hline(yintercept = 0, linetype="dotted") +
  scale_y_continuous(limits=c(-0.5,1)) +
  coord_flip() +
  ylab('Mean Differences (log(TIME))') +
  xlab('Model') + theme_bw() + theme(legend.position="none",
                                     axis.title.x=element_blank(),
                                     axis.text.x=element_text(size=12),
                                     axis.title.y=element_blank(),
                                     axis.text.y=element_blank())

# (2) using back-transformed data for reporting
mean_dur_diff_org <- exp(mean_dur_diff)
mean_dur_diff_lwr_org <- exp(mean_dur_diff_lwr)
mean_dur_diff_upr_org <- exp(mean_dur_diff_upr)

mean_dur_diff_upr_org
mean_dur_diff_org
mean_dur_diff_lwr_org

duration_original_CI=data.frame(contrast="CNL-KV", lower=mean_dur_diff_lwr_org, mean=mean_dur_diff_org, upper=mean_dur_diff_upr_org)

ggplot(data=duration_original_CI) +
  geom_bar(aes(x=contrast, y=mean-1), stat="identity", fill="lightblue", position = position_nudge(y = 1)) +
  geom_pointrange(mapping=aes(x=contrast, y=mean, ymin=lower, ymax=upper), size=1, color="black", fill="white", shape=22) +
  geom_hline(yintercept = 1, linetype="dotted") +
  scale_y_continuous(limits=c(0,2)) +
  coord_flip() +
  ylab('Ratio of mean difference') +
  xlab('CNL-KV') + 
  labs(title = "Ratio of differences for mean levels of duration", subtitle = "Tukey HSD: 95% confidence interval", caption = "Note: Mean differences and CI bounds are back-transformed to original scale as a ratio. \n Line of no effect is shifted to 1.") + theme_bw() + theme(legend.position="none",
                                     axis.title.x=element_text(size=12),
                                     axis.text.x=element_text(size=12),
                                     axis.title.y=element_blank(),
                                     axis.text.y=element_text(size=12))

# Result duration.log (log transformed)
# lwr = 0.1764396 mean difference = 0.2689925 upr = 0.3615455

# Result duration (back transformed)
# lwr = 1.192962 mean difference = 1.308645  upr = 1.435546

# Reporting: KV is im schlechtesten fall 19% schneller & im schnitt 31% und im Besten Fall 44 % fall

```

[**Pairwise T-Test**]{.underline}

Keeping this for reference. These values are not reported in the thesis. However they come to the same conclusion. We run both tests for sake of parametric vs. non-parametric difference. For the variable efficiency (duration) data is normal, therefore we look at result from parametric paired t-test.

```{r}
##### t-test ####
## two-sided, paired t-test
t.test(x = df$duration.log[df$notation.r == "natural language"], y = df$duration.log[df$notation.r == "key-value"], paired = TRUE, conf.level = 0.95, alternative = "two.sided")

##### wilcoxon ####
wilcox.test(df$duration.log[df$notation.r == "natural language"],df$duration.log[df$notation.r == "key-value"], paired = TRUE, conf.int=TRUE, conf.level = 0.95)
```

#### Effectiveness (Accuracy)

##### (1) Linear Mixed Model

[**Model 1: Sequence Effect**]{.underline}

```{r}
###### Linear Regression (Linear Mixed Model) for Accuracy ####
# based on input from Thomas Rusch + slideset from Meier
# model as a sequence effect
model_seq_acc<-lmer(accuracy~notation.r+period+sequence+(1|subject),data=df)
summary(model_seq_acc)
```

From the above we can see that there is no period effect. Note that H0 refers to the period effect.

| Parameter     | Estimate | Standard Error | T-Value | DF  | Prob Level | Reject H0 at α = 0.050? |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| Period Effect | 3.305    | 5.005          | 0.660   | 46  | 0.5124     | No                      |

[Report:]{.underline} The tests of fixed effects show a **no significant period effect**. The test fails to reject the assumption of equal period effects at the 0.05 significance level (the actual probability level is 0.5124).

[**Model 2: Interaction Effect**]{.underline}

```{r}
# model as a interaction effect
model_int_acc<-lmer(accuracy~notation.r*period+(1|subject),data=df)
summary(model_int_acc)
```

From the above output we can see that there is no carryover effect (the carryover effect is leveled out by design and this is hereby confirmed:

| Parameter        | Estimate | Standard Error | T-Value | DF  | Prob Level | Reject H0 at α = 0.050? |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| Carryover Effect | 23.170   | 12.555         | 1.845   | 46  | 0.0714     | No                      |

[Report:]{.underline} The tests of fixed effects show **no significant carryover effect**. The test failed to reject the assumption of equal carryover effects at the 0.05 significance level (the actual probability level is 0.0714).

[**Treatment Effect: Duration**]{.underline}

| Alternative Hypothesis | Mean Difference in percent | Standard Error | T-Value | DF  | Prob Level | Reject H0 at α = 0.050? |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| μCNL - μKV ≠ 0         | -3.774                     | 5.005          | -0.754  | 46  | 0.4546     | No                      |

Report: The two treatment means are not significantly different based on the reported fixed effects in the linear mixed model at the 0.05 significance level (the actual probability level was 0.4546).

##### (2) Effect Size

[**Unstandardized**]{.underline}

```{r}
##### mean difference (measured values) ####
# values
m_accuracy_nl <- mean(df$accuracy[df$notation.r == "natural language"])
m_accuracy_kv <- mean(df$accuracy[df$notation.r == "key-value"])
# unstandardized mean difference between cnl kv
diff_accuracy_nlkv <- m_accuracy_kv-m_accuracy_nl
# avergae of nl kv
avg_accuracy_nlkv <- (m_accuracy_nl+m_accuracy_kv)/2
ratio_diff_accuracy_nlkv <- diff_accuracy_nlkv/avg_accuracy_nlkv
# percentage
percent_diff_accuracy_nlkv <- ratio_diff_accuracy_nlkv*100
print(c("Mean Difference Accuracy (measured in raw units: percent completed)"))
print(c("mean difference accuracy: ", diff_accuracy_nlkv))
print(c("mean difference in percentage: ", percent_diff_accuracy_nlkv))
```

[Report:]{.underline} The estimated unstandardized mean difference (μCNL - μKV), measured in raw units (percent completed) is 3.77, that is a difference of 4.64 %.

[**Standardized**]{.underline}

**Marginal/Conditional R2**

```{r}
# R-squared (marginal/conditional)
r2_acc <- performance::r2(model_seq_acc)
r2_acc
# Interpretation of R-squared (marginal/conditional)
interpret_r2(c(r2_acc[["R2_conditional"]][["Conditional R2"]],r2_acc[["R2_marginal"]][["Marginal R2"]]), rules = "cohen1988")

# Using partR2 to confirm performance package and get CIs
# Marginal R-squared
r2_mar_acc <- partR2(model_seq_acc, partvars = c("notation.r", "period"), 
                  R2_type = "marginal", nboot = 10)
r2_mar_acc
summary(r2_mar_acc)
forestplot(r2_mar_acc, type = "R2")

# Conditional R-squared
r2_con_acc <- partR2(model_seq_acc, partvars = c("notation.r", "period"), 
                  R2_type = "conditional", nboot = 10)
r2_con_acc
summary(r2_con_acc)
forestplot(r2_con_acc, type = "R2")
```

**Cohen's d**

```{r}
effectsize::cohens_d(df$accuracy[df$notation.r == "natural language"],df$accuracy[df$notation.r == "key-value"], paired = TRUE, pooled_sd = TRUE)
```

##### (3) Posthoc: Tukey HSD

[**Tukey HSD**]{.underline}

Post-hoc test looking at contrast CNL-KV.

```{r}
summary(glht(model_seq_acc, linfct=mcp(notation.r="Tukey")))
ci_acc <- confint(model_seq_acc, level = 0.95)
ci_acc

means <- tapply(df$accuracy, df$notation.r, mean)
mean_acc_diff <- means[1] - means[2]
mean_acc_diff <- mean_acc_diff[["key-value"]]
mean_acc_diff_lwr <- ci_acc[4,1]
mean_acc_diff_upr <- ci_acc[4,2]

mean_acc_diff_upr
mean_acc_diff
mean_acc_diff_lwr

# Plotting Tukey HSD
# Source: Stefan aov.html + http://sape.inf.usi.ch/quick-reference/ggplot2/geom_pointrange
accuracy_CI=data.frame(contrast="CNL-KV", lower=mean_acc_diff_lwr, mean=mean_acc_diff, upper=mean_acc_diff_upr)

ggplot(data=accuracy_CI) +
  geom_bar(aes(x=contrast, y=mean-1), stat="identity", fill="lightblue", position = position_nudge(y = 1)) +
  geom_pointrange(mapping=aes(x=contrast, y=mean, ymin=lower, ymax=upper), size=1, color="black", fill="white", shape=22) +
  geom_hline(yintercept = 1, linetype="dotted") +
  scale_y_continuous(limits=c(-14,7)) +
  coord_flip() +
  ylab('Ratio of mean difference') +
  xlab('CNL-KV') + 
  labs(title = "Ratio of differences for mean levels of accuracy", subtitle = "Tukey HSD: 95% confidence interval", caption = "Note: Mean differences and CI bounds are reported in original scale as ratio. \n Line of no effect is shifted to 1.") + theme_bw() + theme(legend.position="none",
                                     axis.title.x=element_text(size=12),
                                     axis.text.x=element_text(size=12),
                                     axis.title.y=element_blank(),
                                     axis.text.y=element_text(size=12))

# Result duration.log (log transformed)
# lwr = 0.1764396 mean difference = 0.2689925 upr = 0.3615455

# Result duration (back transformed)
# lwr = 1.192962 mean difference = 1.308645  upr = 1.435546

# Reporting: KV is im schlechtesten fall 19% schneller & im schnitt 31% und im Besten Fall 44 % fall

```

[**Pairwise T-Test**]{.underline}

Keeping this for reference. These values are not reported in the thesis. However they come to the same conclusion. We run both tests for sake of parametric vs. non-parametric difference. For the variable effectivness (accuracy) data is not normal, therefore we look at result from non-parametric paired t-test.

```{r}
##### t-test ####
## two-sided, paired t-test
t.test(x = df$accuracy[df$notation.r == "natural language"], y = df$accuracy[df$notation.r == "key-value"], paired = TRUE, conf.level = 0.95, alternative = "two.sided")

##### wilcoxon ####
wilcox.test(df$accuracy[df$notation.r == "natural language"],df$accuracy[df$notation.r == "key-value"], paired = TRUE, conf.int=TRUE, conf.level = 0.95)

```

#### Usability (SUS)

##### (1) Linear Mixed Model

[**Model 1: Sequence Effect**]{.underline}

```{r}
###### Linear Regression (Linear Mixed Model) for Accuracy ####
# based on input from Thomas Rusch + slideset from Meier
# model as a sequence effect
model_seq_sus<-lmer(sus.boxcox~notation.r+period+sequence+(1|subject),data=df)
summary(model_seq_sus)
```

From the above we can see that there is a period effect. Note that H0 refers to the period effect.

| Parameter     | Estimate  | Standard Error | T-Value | DF  | Prob Level | Reject H0 at α = 0.050? |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| Period Effect | -76737.05 | 27193.34       | -2.822  | 46  | 0.007027   | Yes                     |

[Report:]{.underline} The tests of fixed effects show a **significant period effect**. The test rejects the assumption of equal period effects at the 0.05 significance level (the actual probability level is 0.007027).

[**Model 2: Interaction Effect**]{.underline}

```{r}
# model as a interaction effect
model_int_sus<-lmer(sus.boxcox~notation.r*period+(1|subject),data=df)
summary(model_int_sus)
```

From the above output we can see that there is no carryover effect (the carryover effect is leveled out by design and this is hereby confirmed:

| Parameter        | Estimate   | Standard Error | T-Value | DF  | Prob Level | Reject H0 at α = 0.050? |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| Carryover Effect | -128398.99 | 93635.16       | -1.371  | 46  | 0.177      | No                      |

[Report:]{.underline} The tests of fixed effects show **no significant carryover effect**. The test failed to reject the assumption of equal carryover effects at the 0.05 significance level (the actual probability level is 0.177).

[**Treatment Effect: Usability**]{.underline}

| Alternative Hypothesis | Mean Difference in percent | Standard Error | T-Value | DF  | Prob Level | Reject H0 at α = 0.050? |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| μCNL - μKV ≠ 0         | -110224.16                 | 27193.34       | -4.053  | 46  | 0.000193   | Yes                     |

Report: The two treatment means are significantly different based on the reported fixed effects in the linear mixed model at the 0.05 significance level (the actual probability level was 0.000193).

##### (2) Effect Size

[**Unstandardized**]{.underline}

```{r}
##### mean difference (boxcox transformed) ####
# values
m_usability_nl <- mean(df$sus.boxcox[df$notation.r == "natural language"])
m_usability_kv <- mean(df$sus.boxcox[df$notation.r == "key-value"])
# unstandardized mean difference between cnl kv
diff_usability_nlkv <- m_usability_kv-m_usability_nl
# avergae of nl kv
avg_usability_nlkv <- (m_usability_nl+m_usability_kv)/2
ratio_diff_usability_nlkv <- diff_usability_nlkv/avg_usability_nlkv
# percentage
percent_diff_usability_nlkv <- ratio_diff_usability_nlkv*100
print(c("Mean Difference SUS Score (log transformed)"))
print(c("mean difference sus score: ", diff_usability_nlkv))
print(c("mean difference in percentage: ", percent_diff_usability_nlkv))

##### mean difference (measured values) ####
# values
m_usability_nl <- mean(df$sus[df$notation.r == "natural language"])
m_usability_kv <- mean(df$sus[df$notation.r == "key-value"])
# unstandardized mean difference between cnl kv
diff_usability_nlkv <- m_usability_kv-m_usability_nl
# avergae of nl kv
avg_usability_nlkv <- (m_usability_nl+m_usability_kv)/2
ratio_diff_usability_nlkv <- diff_usability_nlkv/avg_usability_nlkv
# percentage
percent_diff_usability_nlkv <- ratio_diff_usability_nlkv*100
print(c("Mean Difference Usability (measured in raw units: SUS Score)"))
print(c("mean difference usability: ", diff_usability_nlkv))
print(c("mean difference in percentage: ", percent_diff_usability_nlkv))
```

[Report:]{.underline} The estimated unstandardized mean difference (μCNL - μKV), measured in raw units (SUS score) is 8.44, that is a difference of 11.16 %.

[**Standardized**]{.underline}

**Marginal/Conditional R2**

"*Marginal R2* considers only the variance of the **fixed effects** (without the random effects) and *conditional R2* takes *both* the **fixed and random effects** into account (i.e., the total model)." ([Source](https://easystats.github.io/performance/articles/r2.html#r2-for-mixed-models))

```{r}
# Understanding effect size for linear mixed models
# Source: https://joshuawiley.com/MonashHonoursStatistics/LMM_Comparison.html#effect-sizes

# Details on getting R-squared for mixed models
# Source: https://easystats.github.io/performance/articles/r2.html#r2-for-mixed-models

# Details on using partR2 to get CIs
# Source: https://cran.r-project.org/web/packages/partR2/vignettes/Using_partR2.html
# Paper: https://peerj.com/articles/11414/

# Interpretation of R-squared
# Source: https://easystats.github.io/effectsize/reference/interpret_r2.html

# R-squared (marginal/conditional)
r2_sus <- performance::r2(model_seq_sus)
r2_sus
# Interpretation of R-suqared (marginal/conditional)
interpret_r2(c(r2_sus[["R2_conditional"]][["Conditional R2"]],r2_sus[["R2_marginal"]][["Marginal R2"]]), rules = "cohen1988")

# Using partR2 to confirm performance package and get CIs
# Marginal R-squared
r2_mar_sus <- partR2(model_seq_sus, partvars = c("notation.r", "period"), 
                  R2_type = "marginal", nboot = 10)
r2_mar_sus
summary(r2_mar_sus)
forestplot(r2_mar_sus, type = "R2")

# Conditional R-squared
r2_con_sus <- partR2(model_seq_sus, partvars = c("notation.r", "period"), 
                  R2_type = "conditional", nboot = 10)
r2_con_sus
summary(r2_con_sus)
forestplot(r2_con_sus, type = "R2")
```

**Cohen's d**

```{r}
effectsize::cohens_d(df$sus.boxcox[df$notation.r == "natural language"],df$sus.boxcox[df$notation.r == "key-value"], paired = TRUE, pooled_sd = TRUE)
```

##### (3) Posthoc: Tukey HSD

[**Tukey HSD**]{.underline}

Post-hoc test looking at contrast CNL-KV.

```{r}
summary(glht(model_seq_sus, linfct=mcp(notation.r="Tukey")))
ci_sus <- confint(model_seq_sus, level = 0.95)

means <- tapply(df$sus.boxcox, df$notation.r, mean)
mean_sus_diff <- means[1] - means[2]
mean_sus_diff <- mean_sus_diff[["key-value"]]
mean_sus_diff_lwr <- ci_sus[4,1] * -1
mean_sus_diff_upr <- ci_sus[4,2] * -1

mean_sus_diff_upr
mean_sus_diff
mean_sus_diff_lwr

# Plotting Tukey HSD
# Source: Stefan aov.html + http://sape.inf.usi.ch/quick-reference/ggplot2/geom_pointrange

# (1) using log transformed data
sus_transformed_CI=data.frame(contrast="nl-kv", lower=mean_sus_diff_lwr, mean=mean_sus_diff, upper=mean_sus_diff_upr)

ggplot() + 
  geom_pointrange(data=sus_transformed_CI, mapping=aes(x=contrast, y=mean, ymin=lower, ymax=upper), size=1, color="black", fill="white", shape=22) + 
  geom_hline(yintercept = 0, linetype="dotted") +
  scale_y_continuous(limits=c(-60000,60000)) +
  coord_flip() +
  ylab('Mean Differences (transformed SUS score)') +
  xlab('Model') + theme_bw() + theme(legend.position="none",
                                     axis.title.x=element_blank(),
                                     axis.text.x=element_text(size=12),
                                     axis.title.y=element_blank(),
                                     axis.text.y=element_blank())

# (2) using back-transformed data for reporting
# Source for back-transforming box-cox: https://www.css.cornell.edu/faculty/dgr2/_static/files/R_html/Transformations.html#3_transformation_and_back-transformation 
mean_sus_diff_org <- exp(log(1 + 3.2 * mean_sus_diff)/3.2)
mean_sus_diff_lwr_org <- exp(log(1 + 3.2 * mean_sus_diff_lwr)/3.2)
mean_sus_diff_upr_org <- exp(log(1 + 3.2 * mean_sus_diff_upr)/3.2)

mean_sus_diff_upr_org
mean_sus_diff_org
mean_sus_diff_lwr_org

sus_original_CI=data.frame(contrast="CNL-KV", lower=mean_sus_diff_lwr_org, mean=mean_sus_diff_org, upper=mean_sus_diff_upr_org)

ggplot(data=sus_original_CI) +
  geom_bar(aes(x=contrast, y=mean-1), stat="identity", fill="lightblue", position = position_nudge(y = 1)) +
  geom_pointrange(mapping=aes(x=contrast, y=mean, ymin=lower, ymax=upper), size=1, color="black", fill="white", shape=22) +
  geom_hline(yintercept = 1, linetype="dotted") +
  scale_y_continuous(limits=c(0,65)) +
  coord_flip() +
  ylab('Ratio of mean difference') +
  xlab('CNL-KV') + 
  labs(title = "Ratio of differences for mean levels of SUS scores", subtitle = "Tukey HSD: 95% confidence interval", caption = "Note: Mean differences and CI bounds are back-transformed to original scale as a ratio. \n Line of no effect is shifted to 1.") + theme_bw() + theme(legend.position="none",
                                     axis.title.x=element_text(size=12),
                                     axis.text.x=element_text(size=12),
                                     axis.title.y=element_blank(),
                                     axis.text.y=element_text(size=12))

# Result sus.boxcox (transformed)
# lwr = 163461.4 mean difference = 110224.2 upr = 56986.89

# Result SUS score (back transformed)
# lwr = 1.192962 mean difference = 1.308645  upr = 1.435546

# Reporting: KV hat im schlechtesten fall 44% bessere Usability & im Schnitt 54% und im Besten Fall 61 % besserer usability.
```

[**Pairwise T-Test**]{.underline}

Keeping this for reference. These values are not reported in the thesis. However they come to the same conclusion. We run both tests for sake of parametric vs. non-parametric difference. For the variable usability (sus score) data is not normal, therefore we look at result from non-parametric paired t-test.

```{r}
##### t-test ####
## two-sided, paired t-test
t.test(x = df$sus.boxcox[df$notation.r == "natural language"], y = df$sus.boxcox[df$notation.r == "key-value"], paired = TRUE, conf.level = 0.95, alternative = "two.sided")

##### wilcoxon ####
wilcox.test(df$sus.boxcox[df$notation.r == "natural language"],df$sus.boxcox[df$notation.r == "key-value"], paired = TRUE, conf.int=TRUE, conf.level = 0.95)

```

#### Ranking (Personal Preference)

```{r}
# Count number of occurences in dfr for each ranking type
count_preferKV <- dim(subset(dfr, preference=='preferKV'))[1]
count_preferNL <- dim(subset(dfr, preference=='preferNL'))[1]
count_nopref <- dim(subset(dfr, preference=='nopref'))[1]
```

##### (1) Binomial Test

Using exact binomial test, ignoring ties (tie = no difference). We only count 33 preferences for the key-value notation and 9 preferences for the controlled natural-language notation. For the record, we have 6 no preference selections (ties). For the binomial test we only count 33 as success and 33+9=42 as total, ignoring no preference.

```{r}
binom_result <- binom.test(count_preferKV,(count_preferKV+count_preferNL), conf.level = 0.95, p = 0.5)
```

[Report:]{.underline} An exact binomial test revealed a significant difference of personal preference towards the key-value notation (p = 0.0002715) with a probability of success of 0.786 and a 95 % confidence interval (0.632,0.897).

**Plotting of confidence interval**

```{r}
binom_lwr_CI <- binom_result[["conf.int"]][1]
binom_sucess <- binom_result[["estimate"]][["probability of success"]]
binom_upr_CI <- binom_result[["conf.int"]][2]


binom_CI=data.frame(contrast="CNL-KV", lower=binom_lwr_CI, mean=binom_sucess, upper=binom_upr_CI)

ggplot(data=binom_CI) +
  geom_bar(aes(x=contrast, y=mean-0.5), stat="identity", fill="lightblue", position = position_nudge(y = 0.5)) +
  geom_pointrange(mapping=aes(x=contrast, y=mean, ymin=lower, ymax=upper), size=1, color="black", fill="white", shape=22) +
  geom_hline(yintercept = 0.5, linetype="dotted") +
  scale_y_continuous(limits=c(-1,2)) +
  coord_flip() +
  ylab('Probability of success') +
  xlab('CNL-KV') + 
  labs(title = "Exact Binomial Test", subtitle = "95% confidence interval", caption = expression("Note: Line of no effect is 0.5 at hypothesized probability of success (see H4"[null]*" = 0.5).")) + theme_bw() + theme(legend.position="none",
                                     axis.title.x=element_text(size=12),
                                     axis.text.x=element_text(size=12),
                                     axis.title.y=element_blank(),
                                     axis.text.y=element_text(size=12))
```

##### (2) Pearson X2

This is the second variant of testing the ranking. Also by ignoring ties. We have the dataframe dfr with 48 rankings as preferKV, preferNL and nopref. First we count each ranking. We explicitly decide against using identicality norms or placebo experiments, because there is a lack of such in the domain of empirical software engineering (compared to e.g. sensory research in food quality and preference). Here, we simply test for a directional effect.

**Calculation of Pearson X2 statistic:**

We are using the calculation illustrated in Christensen et al. (2014).

```{r}
# Calculate Pearson X2
chi_expectedCounts <- ((count_preferKV + count_preferNL)/2)
chi_directional_part1 <- ((count_preferKV - chi_expectedCounts)^2)/chi_expectedCounts
chi_directional_part2 <- ((count_preferNL - chi_expectedCounts)^2)/chi_expectedCounts
chiSquare_directional <- chi_directional_part1 + chi_directional_part2
chiSquare_directional

# Calculate p-value 
p_chi <- pchisq(chiSquare_directional, df=1, lower.tail = FALSE)
p_chi
```

[Report:]{.underline} A Pearson chi-square test for directional effect revealed a significant difference of personal preference towards the key-value notation (chi-square = 13.714, sqrt of chi-square = 3.703 p = 0.0002128294).

##### (3) Thurstonian 2-AC Model

This calculation is fully based on Christensen et al. (2012). The authors present a methodology to calculate estimates for the Thurstonian model for a 2-AC protocol. This is the only calculation that includes the no preference counts.

"The 2-AC protocol is a 2-AFC protocol with a 'no difference' option and is technically identical to the paired preference test with a no-preference' option" (Christensen et al., 2014, p. 119)

Christensen et al. (2012) also provide an implementation of the methodology with the R-package *sensR*. In the below source code we are using sensR with the *twoAC* function. See sensR for more details: [sensR.pdf](https://cran.r-project.org/web/packages/sensR/sensR.pdf) \| [twoACexamples.pdf](https://rdrr.io/cran/sensR/f/inst/doc/twoACexamples.pdf)

This calculation is kept for reference and future analysis.

```{r}
# Format: e form ("prefer A", "no-preference", "preferB").
fit <- twoAC(c(count_preferNL,count_nopref,count_preferKV))
fit$coefficients[2]

pr <- profile(fit, range = c(-1,2))
confint(pr)[2]
plot(pr)

dprime_lwr_CI <- confint(pr)[1]
dprime <- fit$coefficients[2]
dprime_upr_CI <- confint(pr)[2]


dprime_CI=data.frame(contrast="CNL-KV", lower=dprime_lwr_CI, mean=dprime, upper=dprime_upr_CI)

ggplot(data=dprime_CI) +
  geom_bar(aes(x=contrast, y=mean), stat="identity", fill="lightblue", position = position_nudge(y = 0)) +
  geom_pointrange(mapping=aes(x=contrast, y=mean, ymin=lower, ymax=upper), size=1, color="black", fill="white", shape=22) +
  geom_hline(yintercept = 0, linetype="dotted") +
  scale_y_continuous(limits=c(-1,2)) +
  coord_flip() +
  ylab('d.prime') +
  xlab('CNL-KV') + 
  labs(title = "Estimation of the Thurstonian model for 2-AC protocl", subtitle = "95% confidence interval", caption = "Note: ...") + theme_bw() + theme(legend.position="none",
                                     axis.title.x=element_text(size=12),
                                     axis.text.x=element_text(size=12),
                                     axis.title.y=element_blank(),
                                     axis.text.y=element_text(size=12))
```

# References

Christensen, R. H. B., Ennis, J. M., Ennis, D. M., & Brockhoff, P. B. (2014). *Paired preference data with a no-preference option -- Statistical tests for comparison with placebo data*. Food Quality and Preference, 32, 48--55. <https://doi.org/10.1016/j.foodqual.2013.06.005>

Ivanchikj, A., Serbout, S., & Pautasso, C. (2020). From text to visual BPMN process models: Design and evaluation. *Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems*, 229--239. <https://doi.org/10.1145/3365438.3410990>

Jones, B., & Kenward, M. G. (2014). *Design and Analysis of Cross-Over Trials* (3rd Ed.). Chapman and Hall/CRC. <https://doi.org/10.1201/b17537>

Kampenes, V. B., Dybå, T., Hannay, J. E., & Sjøberg, D. I. K. (2007). A systematic review of effect size in software engineering experiments. *Information and Software Technology*, *49*(11), 1073--1086. <https://doi.org/10.1016/j.infsof.2007.02.015>

Vegas, S., Apa, C., & Juristo, N. (2016). Crossover Designs in Software Engineering Experiments: Benefits and Perils. *IEEE Transactions on Software Engineering*, *42*(2), 120--135. <https://doi.org/10.1109/TSE.2015.2467378>
